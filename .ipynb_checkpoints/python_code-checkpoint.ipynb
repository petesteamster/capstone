{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R MICE vs Python MICE \n",
    "This data used in this project is based on data collected for individual residential properties by the Ames Assessors Office. The data was collected between 2006 and 2010. Upto 82 variables were collected for each property. They were used to assist in the computation of the assessed property values. This data will be used\n",
    "to created a model(s) which will ultimately be used to compare the MICE R and MICE Python imputation capabilities \n",
    "To start, we looked at the data types. The code below created a list of the features with null values. The nulls in this data are 'valid nulls' (NA) in that they designate when a variable is not applicable to a property. Take for example 'Alley'. If a home has no alley, the type of alley is 'NA\" or not applicable. If a property has no 'lot frontage', that varible is nat applicable to that property. Since 'lot frontage' is a numeric value, we decided to set 'NA\" to zero in this case. Where 'NA' is found in an irrelevant categorical variable, the 'NA' \n",
    "will be assigned a dummy varible - like all the other categorical values would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 80 columns):\n",
      "MSSubClass       1460 non-null int64\n",
      "MSZoning         1460 non-null object\n",
      "LotFrontage      1460 non-null int64\n",
      "LotArea          1460 non-null int64\n",
      "Street           1460 non-null object\n",
      "Alley            91 non-null object\n",
      "LotShape         1460 non-null object\n",
      "LandContour      1460 non-null object\n",
      "Utilities        1460 non-null object\n",
      "LotConfig        1460 non-null object\n",
      "LandSlope        1460 non-null object\n",
      "Neighborhood     1460 non-null object\n",
      "Condition1       1460 non-null object\n",
      "Condition2       1460 non-null object\n",
      "BldgType         1460 non-null object\n",
      "HouseStyle       1460 non-null object\n",
      "OverallQual      1460 non-null int64\n",
      "OverallCond      1460 non-null int64\n",
      "YearBuilt        1460 non-null int64\n",
      "YearRemodAdd     1460 non-null int64\n",
      "RoofStyle        1460 non-null object\n",
      "RoofMatl         1460 non-null object\n",
      "Exterior1st      1460 non-null object\n",
      "Exterior2nd      1460 non-null object\n",
      "MasVnrType       1452 non-null object\n",
      "MasVnrArea       1452 non-null float64\n",
      "ExterQual        1460 non-null object\n",
      "ExterCond        1460 non-null object\n",
      "Foundation       1460 non-null object\n",
      "BsmtQual         1423 non-null object\n",
      "BsmtCond         1423 non-null object\n",
      "BsmtExposure     1422 non-null object\n",
      "BsmtFinType1     1423 non-null object\n",
      "BsmtFinSF1       1460 non-null int64\n",
      "BsmtFinType2     1422 non-null object\n",
      "BsmtFinSF2       1460 non-null int64\n",
      "BsmtUnfSF        1460 non-null int64\n",
      "TotalBsmtSF      1460 non-null int64\n",
      "Heating          1460 non-null object\n",
      "HeatingQC        1460 non-null object\n",
      "CentralAir       1460 non-null object\n",
      "Electrical       1459 non-null object\n",
      "1stFlrSF         1460 non-null int64\n",
      "2ndFlrSF         1460 non-null int64\n",
      "LowQualFinSF     1460 non-null int64\n",
      "GrLivArea        1460 non-null int64\n",
      "BsmtFullBath     1460 non-null int64\n",
      "BsmtHalfBath     1460 non-null int64\n",
      "FullBath         1460 non-null int64\n",
      "HalfBath         1460 non-null int64\n",
      "BedroomAbvGr     1460 non-null int64\n",
      "KitchenAbvGr     1460 non-null int64\n",
      "KitchenQual      1460 non-null object\n",
      "TotRmsAbvGrd     1460 non-null int64\n",
      "Functional       1460 non-null object\n",
      "Fireplaces       1460 non-null int64\n",
      "FireplaceQu      770 non-null object\n",
      "GarageType       1379 non-null object\n",
      "GarageYrBlt      1379 non-null float64\n",
      "GarageFinish     1379 non-null object\n",
      "GarageCars       1460 non-null int64\n",
      "GarageArea       1460 non-null int64\n",
      "GarageQual       1379 non-null object\n",
      "GarageCond       1379 non-null object\n",
      "PavedDrive       1460 non-null object\n",
      "WoodDeckSF       1460 non-null int64\n",
      "OpenPorchSF      1460 non-null int64\n",
      "EnclosedPorch    1460 non-null int64\n",
      "3SsnPorch        1460 non-null int64\n",
      "ScreenPorch      1460 non-null int64\n",
      "PoolArea         1460 non-null int64\n",
      "PoolQC           7 non-null object\n",
      "Fence            281 non-null object\n",
      "MiscFeature      54 non-null object\n",
      "MiscVal          1460 non-null int64\n",
      "MoSold           1460 non-null int64\n",
      "YrSold           1460 non-null int64\n",
      "SaleType         1460 non-null object\n",
      "SaleCondition    1460 non-null object\n",
      "SalePrice        1460 non-null int64\n",
      "dtypes: float64(2), int64(35), object(43)\n",
      "memory usage: 912.6+ KB\n"
     ]
    }
   ],
   "source": [
    "#the following code reads in the training data and finds all columns with null values and sums them up.\n",
    "#\n",
    "import pandas as pd\n",
    "tFile=\"train.csv\"\n",
    "df1 = pd.read_csv(tFile)\n",
    "df1.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is the code used to train the model. \n",
    "We used lasso. We chose it because we wanted to reduce the number of features. We had a lot of features because we included all 2nd degree polynomials. This led to arrpoximately 45000 features. Before running lasso, we reduced the features by thresholding on correlation. Correlation became a kind of hyperparameter. We found that our model performed the best when we set the correlation threshold to an absolute value of 0.25. In our actual process, we split this code into two sections (see below). We commented it out because it should not be run as a whole. It should be run in sections.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 246)\n",
      "(1460, 245)\n",
      "1000\n",
      "449\n",
      "(449, 153)\n",
      "r2  0.8192507146979593\n"
     ]
    }
   ],
   "source": [
    "# from sklearn import linear_model\n",
    "# from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from xgboost import XGBRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from matplotlib import*\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy import stats\n",
    "# from sklearn.preprocessing import PolynomialFeatures \n",
    "# import seaborn\n",
    "# import random as rd\n",
    "# def split_data(full_data):\n",
    "#     # performs a very simple split. It is called after the data had been randomly shuffled. WE used this method\n",
    "#     # because we were having trouble getting sklearn's split/test to work\n",
    "#     return_list=[] \n",
    "#     return_list.append(list(range(0,1000)))\n",
    "#     return_list.append(list(range(1001,1450)))\n",
    "#     return return_list\n",
    "# def get_important_features(t_features,t_bool):\n",
    "#     # extracts features based on true/false values in t_bool \n",
    "#     t_width=t_features.shape[1]\n",
    "#     t_range=list(range(0,t_width))\n",
    "#     t_bool2=pd.concat([pd.Series(t_bool),pd.Series(t_range)],axis=1)\n",
    "#     t_bool3=t_bool2[t_bool]\n",
    "#     t_good=list(t_bool3.iloc[:,1])\n",
    "#     tF_good=t_features.iloc[:,t_good]\n",
    "#     return (tF_good)\n",
    "# def get_features_filtered_on_corr(t_f,t_corr,t_thresh):\n",
    "#     # extracts features based on a correlation threshold\n",
    "#     t_width=t_f.shape[1]\n",
    "#     tB=pd.DataFrame(np.asarray(t_corr),columns=['corr'])\n",
    "#     tB=tB.fillna(0)\n",
    "#     t_bool=tB['corr']>t_thresh\n",
    "#     t_range=list(range(0,t_width))\n",
    "#     t_bool2=pd.concat([pd.Series(t_bool),pd.Series(t_range)],axis=1)\n",
    "#     t_bool3=t_bool2[t_bool]\n",
    "#     t_good=list(t_bool3.iloc[:,1])\n",
    "#     tF_good=t_f.iloc[:,t_good]\n",
    "#     return (tF_good)\n",
    "# def get_corr(t_df,t_target):\n",
    "#     # calculates correlations if a set of features and returns those correlations in a list  \n",
    "#     import pandas as pd\n",
    "#     t_corr_list=list() \n",
    "#     t_col_count=t_df.shape[1]\n",
    "#     for i in range(0,t_col_count):\n",
    "#         #print('in corr')\n",
    "#         t_corr_list.append(t_df.iloc[:,i].corr(t_target))\n",
    "#     return t_corr_list\n",
    "# t_corr_thresh=.25            \n",
    "# tFile=\"train.csv\"\n",
    "\n",
    "# ## read in the training data \n",
    "# df1 = pd.read_csv(tFile)\n",
    "# df1=pd.get_dummies(df1,drop_first=True)\n",
    "# print(df1.shape)\n",
    "# t_length=len(df1)\n",
    "# #used to shuffle data between runs to get a random split between train test \n",
    "# df1 = df1.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# poly =  PolynomialFeatures(degree=2,interaction_only=False)\n",
    "# t_range=np.logspace(-5,1,3)\n",
    "\n",
    "# df2=df1.drop(columns=[\"SalePrice\"],axis=1)\n",
    "# t_target=df1[\"SalePrice\"]\n",
    "# #\n",
    "# a_train=df2 \n",
    "# a_train=a_train.fillna(0)\n",
    "# b_train=t_target\n",
    "# scaler = StandardScaler()\n",
    "# #simple split. For each run, the data is shuffled, so we only need a simple split. \n",
    "# split_list=split_data(a_train)\n",
    "# df2_scaler=scaler.fit(a_train.iloc[split_list[0],:])\n",
    "# a_train2=df2_scaler.transform(a_train.iloc[split_list[0],:])\n",
    "# #reg = LassoCV(cv=7)\n",
    "# #reg = XGBRegressor()\n",
    "# reg = RandomForestRegressor(n_estimators=1000)\n",
    "# tF=pd.DataFrame(a_train2)\n",
    "# tF=pd.DataFrame(poly.fit_transform(a_train2))\n",
    "# tF=pd.DataFrame(tF)\n",
    "# tL=get_corr(tF,b_train[split_list[0]])\n",
    "# t_f_subset=get_features_filtered_on_corr(tF,tL,t_corr_thresh)\n",
    "# reg.fit(t_f_subset,b_train[split_list[0]])\n",
    "# a_train2=df2_scaler.transform(a_train.iloc[split_list[1],:])\n",
    "# a_train2=pd.DataFrame(poly.fit_transform(a_train2))\n",
    "# print(a_train.shape)\n",
    "# tF=pd.DataFrame(a_train2)\n",
    "# t_f_subset=get_features_filtered_on_corr(tF,tL,t_corr_thresh)\n",
    "\n",
    "# tS=reg.score(t_f_subset,b_train[split_list[1]])\n",
    "# print(len(split_list[0]))\n",
    "# print(len(split_list[1]))\n",
    "# print(t_f_subset.shape)\n",
    "# print('r2 ',str(tS))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is the portion of the code that creates the features and filters based on correlation with the target. It outputs the processed data so R and ampute it. Ampute is the process that takes in data and creates a new set of data that is a  missing data simulation. This ampute file (from R) is put into a CSV so the Python verson of MICE can be run on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 246)\n",
      "(1000, 172)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import*\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "import seaborn\n",
    "import random as rd\n",
    "def split_data(full_data):\n",
    "    # performs a very simple split. It is called after the data had been randomly shuffled. WE used this method\n",
    "    # because we were having trouble getting sklearn's split/test to work\n",
    "    return_list=[] \n",
    "    return_list.append(list(range(0,1000)))\n",
    "    return_list.append(list(range(1001,1450)))\n",
    "    return return_list\n",
    "def get_important_features(t_features,t_bool):\n",
    "    # extracts features based on true/false values in t_bool \n",
    "    t_width=t_features.shape[1]\n",
    "    t_range=list(range(0,t_width))\n",
    "    t_bool2=pd.concat([pd.Series(t_bool),pd.Series(t_range)],axis=1)\n",
    "    t_bool3=t_bool2[t_bool]\n",
    "    t_good=list(t_bool3.iloc[:,1])\n",
    "    tF_good=t_features.iloc[:,t_good]\n",
    "    return (tF_good)\n",
    "def get_features_filtered_on_corr(t_f,t_corr,t_thresh):\n",
    "    # extracts features based on a correlation threshold\n",
    "    t_width=t_f.shape[1]\n",
    "    tB=pd.DataFrame(np.asarray(t_corr),columns=['corr'])\n",
    "    tB=tB.fillna(0)\n",
    "    t_bool=tB['corr']>t_thresh\n",
    "    t_range=list(range(0,t_width))\n",
    "    t_bool2=pd.concat([pd.Series(t_bool),pd.Series(t_range)],axis=1)\n",
    "    t_bool3=t_bool2[t_bool]\n",
    "    t_good=list(t_bool3.iloc[:,1])\n",
    "    tF_good=t_f.iloc[:,t_good]\n",
    "    return (tF_good)\n",
    "def get_corr(t_df,t_target):\n",
    "    # calculates correlations if a set of features and returns those correlations in a list  \n",
    "    import pandas as pd\n",
    "    t_corr_list=list() \n",
    "    t_col_count=t_df.shape[1]\n",
    "    for i in range(0,t_col_count):\n",
    "        #print('in corr')\n",
    "        t_corr_list.append(t_df.iloc[:,i].corr(t_target))\n",
    "    return t_corr_list\n",
    "t_corr_thresh=.25            \n",
    "\n",
    "tFile=\"train.csv\"\n",
    "\n",
    "## read in the training data \n",
    "df1 = pd.read_csv(tFile)\n",
    "df1=pd.get_dummies(df1,drop_first=True)\n",
    "print(df1.shape)\n",
    "t_length=len(df1)\n",
    "#used to shuffle data between runs to get a random split between train test \n",
    "df1 = df1.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "poly =  PolynomialFeatures(degree=2,interaction_only=False)\n",
    "t_range=np.logspace(-5,1,3)\n",
    "\n",
    "df2=df1.drop(columns=[\"SalePrice\"],axis=1)\n",
    "t_target=df1[\"SalePrice\"]\n",
    "#\n",
    "a_train=df2 \n",
    "a_train=a_train.fillna(0)\n",
    "b_train=t_target\n",
    "scaler = StandardScaler()\n",
    "#simple split. For each run, the data is shuffled, so we only need a simple split. \n",
    "split_list=split_data(a_train)\n",
    "df2_scaler=scaler.fit(a_train.iloc[split_list[0],:])\n",
    "a_train2=df2_scaler.transform(a_train.iloc[split_list[0],:])\n",
    "#reg = LassoCV(cv=7)\n",
    "#reg = XGBRegressor()\n",
    "reg = RandomForestRegressor(n_estimators=1000)\n",
    "tF=pd.DataFrame(a_train2)\n",
    "tF=pd.DataFrame(poly.fit_transform(a_train2))\n",
    "tF=pd.DataFrame(tF)\n",
    "tL=get_corr(tF,b_train[split_list[0]])\n",
    "t_f_subset=get_features_filtered_on_corr(tF,tL,t_corr_thresh)\n",
    "subset_corr=get_corr(t_f_subset,b_train[split_list[0]])\n",
    "subset_cols=t_f_subset.columns.values.tolist()\n",
    "#display(subset_cols)\n",
    "for i in range(len(subset_cols)):\n",
    "    subset_cols[i]='col_'+str(subset_cols[i])\n",
    "t_f_subset.columns=subset_cols    \n",
    "corr_df=pd.DataFrame(subset_corr,columns=['corr'])\n",
    "corr_df.to_csv('correlation_coefs.csv',index=False)\n",
    "t_f_subset.to_csv('training_data_for_ampute.csv',index=False)\n",
    "##\n",
    "##  Here is where the datafile is created for the ampute process\n",
    "##\n",
    "##\n",
    "print(t_f_subset.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is the code that performs the python impute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 172)\n",
      "[MICE] Completing matrix with shape (1000, 172)\n",
      "[MICE] Starting imputation round 1/110, elapsed time 0.004\n",
      "[MICE] Starting imputation round 2/110, elapsed time 0.247\n",
      "[MICE] Starting imputation round 3/110, elapsed time 0.433\n",
      "[MICE] Starting imputation round 4/110, elapsed time 0.610\n",
      "[MICE] Starting imputation round 5/110, elapsed time 0.757\n",
      "[MICE] Starting imputation round 6/110, elapsed time 0.931\n",
      "[MICE] Starting imputation round 7/110, elapsed time 1.075\n",
      "[MICE] Starting imputation round 8/110, elapsed time 1.241\n",
      "[MICE] Starting imputation round 9/110, elapsed time 1.384\n",
      "[MICE] Starting imputation round 10/110, elapsed time 1.553\n",
      "[MICE] Starting imputation round 11/110, elapsed time 1.695\n",
      "[MICE] Starting imputation round 12/110, elapsed time 1.865\n",
      "[MICE] Starting imputation round 13/110, elapsed time 2.009\n",
      "[MICE] Starting imputation round 14/110, elapsed time 2.184\n",
      "[MICE] Starting imputation round 15/110, elapsed time 2.330\n",
      "[MICE] Starting imputation round 16/110, elapsed time 2.503\n",
      "[MICE] Starting imputation round 17/110, elapsed time 2.647\n",
      "[MICE] Starting imputation round 18/110, elapsed time 2.838\n",
      "[MICE] Starting imputation round 19/110, elapsed time 2.983\n",
      "[MICE] Starting imputation round 20/110, elapsed time 3.155\n",
      "[MICE] Starting imputation round 21/110, elapsed time 3.298\n",
      "[MICE] Starting imputation round 22/110, elapsed time 3.471\n",
      "[MICE] Starting imputation round 23/110, elapsed time 3.612\n",
      "[MICE] Starting imputation round 24/110, elapsed time 3.782\n",
      "[MICE] Starting imputation round 25/110, elapsed time 3.924\n",
      "[MICE] Starting imputation round 26/110, elapsed time 4.097\n",
      "[MICE] Starting imputation round 27/110, elapsed time 4.240\n",
      "[MICE] Starting imputation round 28/110, elapsed time 4.411\n",
      "[MICE] Starting imputation round 29/110, elapsed time 4.554\n",
      "[MICE] Starting imputation round 30/110, elapsed time 4.725\n",
      "[MICE] Starting imputation round 31/110, elapsed time 4.870\n",
      "[MICE] Starting imputation round 32/110, elapsed time 5.040\n",
      "[MICE] Starting imputation round 33/110, elapsed time 5.182\n",
      "[MICE] Starting imputation round 34/110, elapsed time 5.350\n",
      "[MICE] Starting imputation round 35/110, elapsed time 5.493\n",
      "[MICE] Starting imputation round 36/110, elapsed time 5.661\n",
      "[MICE] Starting imputation round 37/110, elapsed time 5.805\n",
      "[MICE] Starting imputation round 38/110, elapsed time 5.973\n",
      "[MICE] Starting imputation round 39/110, elapsed time 6.119\n",
      "[MICE] Starting imputation round 40/110, elapsed time 6.289\n",
      "[MICE] Starting imputation round 41/110, elapsed time 6.440\n",
      "[MICE] Starting imputation round 42/110, elapsed time 6.609\n",
      "[MICE] Starting imputation round 43/110, elapsed time 6.750\n",
      "[MICE] Starting imputation round 44/110, elapsed time 6.962\n",
      "[MICE] Starting imputation round 45/110, elapsed time 7.130\n",
      "[MICE] Starting imputation round 46/110, elapsed time 7.299\n",
      "[MICE] Starting imputation round 47/110, elapsed time 7.441\n",
      "[MICE] Starting imputation round 48/110, elapsed time 7.610\n",
      "[MICE] Starting imputation round 49/110, elapsed time 7.754\n",
      "[MICE] Starting imputation round 50/110, elapsed time 7.924\n",
      "[MICE] Starting imputation round 51/110, elapsed time 8.067\n",
      "[MICE] Starting imputation round 52/110, elapsed time 8.238\n",
      "[MICE] Starting imputation round 53/110, elapsed time 8.381\n",
      "[MICE] Starting imputation round 54/110, elapsed time 8.550\n",
      "[MICE] Starting imputation round 55/110, elapsed time 8.691\n",
      "[MICE] Starting imputation round 56/110, elapsed time 8.883\n",
      "[MICE] Starting imputation round 57/110, elapsed time 9.027\n",
      "[MICE] Starting imputation round 58/110, elapsed time 9.197\n",
      "[MICE] Starting imputation round 59/110, elapsed time 9.351\n",
      "[MICE] Starting imputation round 60/110, elapsed time 9.524\n",
      "[MICE] Starting imputation round 61/110, elapsed time 9.667\n",
      "[MICE] Starting imputation round 62/110, elapsed time 9.837\n",
      "[MICE] Starting imputation round 63/110, elapsed time 9.982\n",
      "[MICE] Starting imputation round 64/110, elapsed time 10.151\n",
      "[MICE] Starting imputation round 65/110, elapsed time 10.295\n",
      "[MICE] Starting imputation round 66/110, elapsed time 10.466\n",
      "[MICE] Starting imputation round 67/110, elapsed time 10.610\n",
      "[MICE] Starting imputation round 68/110, elapsed time 10.782\n",
      "[MICE] Starting imputation round 69/110, elapsed time 10.927\n",
      "[MICE] Starting imputation round 70/110, elapsed time 11.098\n",
      "[MICE] Starting imputation round 71/110, elapsed time 11.239\n",
      "[MICE] Starting imputation round 72/110, elapsed time 11.408\n",
      "[MICE] Starting imputation round 73/110, elapsed time 11.551\n",
      "[MICE] Starting imputation round 74/110, elapsed time 11.722\n",
      "[MICE] Starting imputation round 75/110, elapsed time 11.865\n",
      "[MICE] Starting imputation round 76/110, elapsed time 12.040\n",
      "[MICE] Starting imputation round 77/110, elapsed time 12.186\n",
      "[MICE] Starting imputation round 78/110, elapsed time 12.360\n",
      "[MICE] Starting imputation round 79/110, elapsed time 12.503\n",
      "[MICE] Starting imputation round 80/110, elapsed time 12.676\n",
      "[MICE] Starting imputation round 81/110, elapsed time 12.818\n",
      "[MICE] Starting imputation round 82/110, elapsed time 13.012\n",
      "[MICE] Starting imputation round 83/110, elapsed time 13.156\n",
      "[MICE] Starting imputation round 84/110, elapsed time 13.326\n",
      "[MICE] Starting imputation round 85/110, elapsed time 13.470\n",
      "[MICE] Starting imputation round 86/110, elapsed time 13.659\n",
      "[MICE] Starting imputation round 87/110, elapsed time 13.802\n",
      "[MICE] Starting imputation round 88/110, elapsed time 13.976\n",
      "[MICE] Starting imputation round 89/110, elapsed time 14.119\n",
      "[MICE] Starting imputation round 90/110, elapsed time 14.290\n",
      "[MICE] Starting imputation round 91/110, elapsed time 14.433\n",
      "[MICE] Starting imputation round 92/110, elapsed time 14.603\n",
      "[MICE] Starting imputation round 93/110, elapsed time 14.746\n",
      "[MICE] Starting imputation round 94/110, elapsed time 14.941\n",
      "[MICE] Starting imputation round 95/110, elapsed time 15.084\n",
      "[MICE] Starting imputation round 96/110, elapsed time 15.255\n",
      "[MICE] Starting imputation round 97/110, elapsed time 15.397\n",
      "[MICE] Starting imputation round 98/110, elapsed time 15.568\n",
      "[MICE] Starting imputation round 99/110, elapsed time 15.712\n",
      "[MICE] Starting imputation round 100/110, elapsed time 15.880\n",
      "[MICE] Starting imputation round 101/110, elapsed time 16.025\n",
      "[MICE] Starting imputation round 102/110, elapsed time 16.195\n",
      "[MICE] Starting imputation round 103/110, elapsed time 16.340\n",
      "[MICE] Starting imputation round 104/110, elapsed time 16.510\n",
      "[MICE] Starting imputation round 105/110, elapsed time 16.654\n",
      "[MICE] Starting imputation round 106/110, elapsed time 16.851\n",
      "[MICE] Starting imputation round 107/110, elapsed time 16.996\n",
      "[MICE] Starting imputation round 108/110, elapsed time 17.164\n",
      "[MICE] Starting imputation round 109/110, elapsed time 17.307\n",
      "[MICE] Starting imputation round 110/110, elapsed time 17.474\n",
      "(1000, 172)\n",
      "[MICE] Completing matrix with shape (1000, 172)\n",
      "[MICE] Starting imputation round 1/110, elapsed time 0.004\n",
      "[MICE] Starting imputation round 2/110, elapsed time 0.161\n",
      "[MICE] Starting imputation round 3/110, elapsed time 0.335\n",
      "[MICE] Starting imputation round 4/110, elapsed time 0.488\n",
      "[MICE] Starting imputation round 5/110, elapsed time 0.663\n",
      "[MICE] Starting imputation round 6/110, elapsed time 0.817\n",
      "[MICE] Starting imputation round 7/110, elapsed time 1.009\n",
      "[MICE] Starting imputation round 8/110, elapsed time 1.171\n",
      "[MICE] Starting imputation round 9/110, elapsed time 1.346\n",
      "[MICE] Starting imputation round 10/110, elapsed time 1.500\n",
      "[MICE] Starting imputation round 11/110, elapsed time 1.675\n",
      "[MICE] Starting imputation round 12/110, elapsed time 1.830\n",
      "[MICE] Starting imputation round 13/110, elapsed time 2.006\n",
      "[MICE] Starting imputation round 14/110, elapsed time 2.160\n",
      "[MICE] Starting imputation round 15/110, elapsed time 2.335\n",
      "[MICE] Starting imputation round 16/110, elapsed time 2.491\n",
      "[MICE] Starting imputation round 17/110, elapsed time 2.687\n",
      "[MICE] Starting imputation round 18/110, elapsed time 2.840\n",
      "[MICE] Starting imputation round 19/110, elapsed time 3.017\n",
      "[MICE] Starting imputation round 20/110, elapsed time 3.171\n",
      "[MICE] Starting imputation round 21/110, elapsed time 3.351\n",
      "[MICE] Starting imputation round 22/110, elapsed time 3.506\n",
      "[MICE] Starting imputation round 23/110, elapsed time 3.686\n",
      "[MICE] Starting imputation round 24/110, elapsed time 3.840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MICE] Starting imputation round 25/110, elapsed time 4.017\n",
      "[MICE] Starting imputation round 26/110, elapsed time 4.173\n",
      "[MICE] Starting imputation round 27/110, elapsed time 4.350\n",
      "[MICE] Starting imputation round 28/110, elapsed time 4.507\n",
      "[MICE] Starting imputation round 29/110, elapsed time 4.694\n",
      "[MICE] Starting imputation round 30/110, elapsed time 4.849\n",
      "[MICE] Starting imputation round 31/110, elapsed time 5.021\n",
      "[MICE] Starting imputation round 32/110, elapsed time 5.174\n",
      "[MICE] Starting imputation round 33/110, elapsed time 5.345\n",
      "[MICE] Starting imputation round 34/110, elapsed time 5.497\n",
      "[MICE] Starting imputation round 35/110, elapsed time 5.670\n",
      "[MICE] Starting imputation round 36/110, elapsed time 5.824\n",
      "[MICE] Starting imputation round 37/110, elapsed time 5.994\n",
      "[MICE] Starting imputation round 38/110, elapsed time 6.147\n",
      "[MICE] Starting imputation round 39/110, elapsed time 6.320\n",
      "[MICE] Starting imputation round 40/110, elapsed time 6.474\n",
      "[MICE] Starting imputation round 41/110, elapsed time 6.660\n",
      "[MICE] Starting imputation round 42/110, elapsed time 6.813\n",
      "[MICE] Starting imputation round 43/110, elapsed time 6.985\n",
      "[MICE] Starting imputation round 44/110, elapsed time 7.137\n",
      "[MICE] Starting imputation round 45/110, elapsed time 7.308\n",
      "[MICE] Starting imputation round 46/110, elapsed time 7.463\n",
      "[MICE] Starting imputation round 47/110, elapsed time 7.660\n",
      "[MICE] Starting imputation round 48/110, elapsed time 7.915\n",
      "[MICE] Starting imputation round 49/110, elapsed time 8.071\n",
      "[MICE] Starting imputation round 50/110, elapsed time 8.243\n",
      "[MICE] Starting imputation round 51/110, elapsed time 8.396\n",
      "[MICE] Starting imputation round 52/110, elapsed time 8.681\n",
      "[MICE] Starting imputation round 53/110, elapsed time 8.895\n",
      "[MICE] Starting imputation round 54/110, elapsed time 9.122\n",
      "[MICE] Starting imputation round 55/110, elapsed time 9.338\n",
      "[MICE] Starting imputation round 56/110, elapsed time 9.519\n",
      "[MICE] Starting imputation round 57/110, elapsed time 9.697\n",
      "[MICE] Starting imputation round 58/110, elapsed time 9.965\n",
      "[MICE] Starting imputation round 59/110, elapsed time 10.118\n",
      "[MICE] Starting imputation round 60/110, elapsed time 10.291\n",
      "[MICE] Starting imputation round 61/110, elapsed time 10.445\n",
      "[MICE] Starting imputation round 62/110, elapsed time 10.618\n",
      "[MICE] Starting imputation round 63/110, elapsed time 10.770\n",
      "[MICE] Starting imputation round 64/110, elapsed time 10.946\n",
      "[MICE] Starting imputation round 65/110, elapsed time 11.102\n",
      "[MICE] Starting imputation round 66/110, elapsed time 11.271\n",
      "[MICE] Starting imputation round 67/110, elapsed time 11.423\n",
      "[MICE] Starting imputation round 68/110, elapsed time 11.594\n",
      "[MICE] Starting imputation round 69/110, elapsed time 11.746\n",
      "[MICE] Starting imputation round 70/110, elapsed time 11.918\n",
      "[MICE] Starting imputation round 71/110, elapsed time 12.072\n",
      "[MICE] Starting imputation round 72/110, elapsed time 12.245\n",
      "[MICE] Starting imputation round 73/110, elapsed time 12.398\n",
      "[MICE] Starting imputation round 74/110, elapsed time 12.569\n",
      "[MICE] Starting imputation round 75/110, elapsed time 12.724\n",
      "[MICE] Starting imputation round 76/110, elapsed time 12.897\n",
      "[MICE] Starting imputation round 77/110, elapsed time 13.050\n",
      "[MICE] Starting imputation round 78/110, elapsed time 13.223\n",
      "[MICE] Starting imputation round 79/110, elapsed time 13.375\n",
      "[MICE] Starting imputation round 80/110, elapsed time 13.547\n",
      "[MICE] Starting imputation round 81/110, elapsed time 13.700\n",
      "[MICE] Starting imputation round 82/110, elapsed time 13.874\n",
      "[MICE] Starting imputation round 83/110, elapsed time 14.027\n",
      "[MICE] Starting imputation round 84/110, elapsed time 14.198\n",
      "[MICE] Starting imputation round 85/110, elapsed time 14.352\n",
      "[MICE] Starting imputation round 86/110, elapsed time 14.526\n",
      "[MICE] Starting imputation round 87/110, elapsed time 14.677\n",
      "[MICE] Starting imputation round 88/110, elapsed time 14.864\n",
      "[MICE] Starting imputation round 89/110, elapsed time 15.017\n",
      "[MICE] Starting imputation round 90/110, elapsed time 15.190\n",
      "[MICE] Starting imputation round 91/110, elapsed time 15.342\n",
      "[MICE] Starting imputation round 92/110, elapsed time 15.515\n",
      "[MICE] Starting imputation round 93/110, elapsed time 15.669\n",
      "[MICE] Starting imputation round 94/110, elapsed time 15.842\n",
      "[MICE] Starting imputation round 95/110, elapsed time 15.997\n",
      "[MICE] Starting imputation round 96/110, elapsed time 16.171\n",
      "[MICE] Starting imputation round 97/110, elapsed time 16.324\n",
      "[MICE] Starting imputation round 98/110, elapsed time 16.499\n",
      "[MICE] Starting imputation round 99/110, elapsed time 16.650\n",
      "[MICE] Starting imputation round 100/110, elapsed time 16.842\n",
      "[MICE] Starting imputation round 101/110, elapsed time 16.995\n",
      "[MICE] Starting imputation round 102/110, elapsed time 17.165\n",
      "[MICE] Starting imputation round 103/110, elapsed time 17.319\n",
      "[MICE] Starting imputation round 104/110, elapsed time 17.491\n",
      "[MICE] Starting imputation round 105/110, elapsed time 17.643\n",
      "[MICE] Starting imputation round 106/110, elapsed time 17.825\n",
      "[MICE] Starting imputation round 107/110, elapsed time 17.978\n",
      "[MICE] Starting imputation round 108/110, elapsed time 18.150\n",
      "[MICE] Starting imputation round 109/110, elapsed time 18.302\n",
      "[MICE] Starting imputation round 110/110, elapsed time 18.475\n",
      "(1000, 172)\n",
      "[MICE] Completing matrix with shape (1000, 172)\n",
      "[MICE] Starting imputation round 1/110, elapsed time 0.004\n",
      "[MICE] Starting imputation round 2/110, elapsed time 0.166\n",
      "[MICE] Starting imputation round 3/110, elapsed time 0.343\n",
      "[MICE] Starting imputation round 4/110, elapsed time 0.497\n",
      "[MICE] Starting imputation round 5/110, elapsed time 0.673\n",
      "[MICE] Starting imputation round 6/110, elapsed time 0.829\n",
      "[MICE] Starting imputation round 7/110, elapsed time 1.021\n",
      "[MICE] Starting imputation round 8/110, elapsed time 1.187\n",
      "[MICE] Starting imputation round 9/110, elapsed time 1.384\n",
      "[MICE] Starting imputation round 10/110, elapsed time 1.541\n",
      "[MICE] Starting imputation round 11/110, elapsed time 1.715\n",
      "[MICE] Starting imputation round 12/110, elapsed time 1.872\n",
      "[MICE] Starting imputation round 13/110, elapsed time 2.049\n",
      "[MICE] Starting imputation round 14/110, elapsed time 2.205\n",
      "[MICE] Starting imputation round 15/110, elapsed time 2.382\n",
      "[MICE] Starting imputation round 16/110, elapsed time 2.537\n",
      "[MICE] Starting imputation round 17/110, elapsed time 2.713\n",
      "[MICE] Starting imputation round 18/110, elapsed time 2.870\n",
      "[MICE] Starting imputation round 19/110, elapsed time 3.044\n",
      "[MICE] Starting imputation round 20/110, elapsed time 3.200\n",
      "[MICE] Starting imputation round 21/110, elapsed time 3.372\n",
      "[MICE] Starting imputation round 22/110, elapsed time 3.528\n",
      "[MICE] Starting imputation round 23/110, elapsed time 3.704\n",
      "[MICE] Starting imputation round 24/110, elapsed time 3.860\n",
      "[MICE] Starting imputation round 25/110, elapsed time 4.035\n",
      "[MICE] Starting imputation round 26/110, elapsed time 4.193\n",
      "[MICE] Starting imputation round 27/110, elapsed time 4.367\n",
      "[MICE] Starting imputation round 28/110, elapsed time 4.523\n",
      "[MICE] Starting imputation round 29/110, elapsed time 4.698\n",
      "[MICE] Starting imputation round 30/110, elapsed time 4.854\n",
      "[MICE] Starting imputation round 31/110, elapsed time 5.029\n",
      "[MICE] Starting imputation round 32/110, elapsed time 5.185\n",
      "[MICE] Starting imputation round 33/110, elapsed time 5.359\n",
      "[MICE] Starting imputation round 34/110, elapsed time 5.515\n",
      "[MICE] Starting imputation round 35/110, elapsed time 5.698\n",
      "[MICE] Starting imputation round 36/110, elapsed time 5.856\n",
      "[MICE] Starting imputation round 37/110, elapsed time 6.032\n",
      "[MICE] Starting imputation round 38/110, elapsed time 6.189\n",
      "[MICE] Starting imputation round 39/110, elapsed time 6.372\n",
      "[MICE] Starting imputation round 40/110, elapsed time 6.552\n",
      "[MICE] Starting imputation round 41/110, elapsed time 6.759\n",
      "[MICE] Starting imputation round 42/110, elapsed time 6.932\n",
      "[MICE] Starting imputation round 43/110, elapsed time 7.131\n",
      "[MICE] Starting imputation round 44/110, elapsed time 7.293\n",
      "[MICE] Starting imputation round 45/110, elapsed time 7.467\n",
      "[MICE] Starting imputation round 46/110, elapsed time 7.622\n",
      "[MICE] Starting imputation round 47/110, elapsed time 7.814\n",
      "[MICE] Starting imputation round 48/110, elapsed time 7.982\n",
      "[MICE] Starting imputation round 49/110, elapsed time 8.190\n",
      "[MICE] Starting imputation round 50/110, elapsed time 8.349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MICE] Starting imputation round 51/110, elapsed time 8.554\n",
      "[MICE] Starting imputation round 52/110, elapsed time 8.740\n",
      "[MICE] Starting imputation round 53/110, elapsed time 8.940\n",
      "[MICE] Starting imputation round 54/110, elapsed time 9.107\n",
      "[MICE] Starting imputation round 55/110, elapsed time 9.296\n",
      "[MICE] Starting imputation round 56/110, elapsed time 9.491\n",
      "[MICE] Starting imputation round 57/110, elapsed time 9.782\n",
      "[MICE] Starting imputation round 58/110, elapsed time 9.945\n",
      "[MICE] Starting imputation round 59/110, elapsed time 10.309\n",
      "[MICE] Starting imputation round 60/110, elapsed time 10.684\n",
      "[MICE] Starting imputation round 61/110, elapsed time 11.043\n",
      "[MICE] Starting imputation round 62/110, elapsed time 11.353\n",
      "[MICE] Starting imputation round 63/110, elapsed time 11.514\n",
      "[MICE] Starting imputation round 64/110, elapsed time 11.753\n",
      "[MICE] Starting imputation round 65/110, elapsed time 11.923\n",
      "[MICE] Starting imputation round 66/110, elapsed time 12.140\n",
      "[MICE] Starting imputation round 67/110, elapsed time 12.394\n",
      "[MICE] Starting imputation round 68/110, elapsed time 12.652\n",
      "[MICE] Starting imputation round 69/110, elapsed time 12.831\n",
      "[MICE] Starting imputation round 70/110, elapsed time 13.039\n",
      "[MICE] Starting imputation round 71/110, elapsed time 13.255\n",
      "[MICE] Starting imputation round 72/110, elapsed time 13.507\n",
      "[MICE] Starting imputation round 73/110, elapsed time 13.806\n",
      "[MICE] Starting imputation round 74/110, elapsed time 13.979\n",
      "[MICE] Starting imputation round 75/110, elapsed time 14.177\n",
      "[MICE] Starting imputation round 76/110, elapsed time 14.334\n",
      "[MICE] Starting imputation round 77/110, elapsed time 14.516\n",
      "[MICE] Starting imputation round 78/110, elapsed time 14.683\n",
      "[MICE] Starting imputation round 79/110, elapsed time 14.878\n",
      "[MICE] Starting imputation round 80/110, elapsed time 15.082\n",
      "[MICE] Starting imputation round 81/110, elapsed time 15.278\n",
      "[MICE] Starting imputation round 82/110, elapsed time 15.442\n",
      "[MICE] Starting imputation round 83/110, elapsed time 15.625\n",
      "[MICE] Starting imputation round 84/110, elapsed time 15.793\n",
      "[MICE] Starting imputation round 85/110, elapsed time 15.968\n",
      "[MICE] Starting imputation round 86/110, elapsed time 16.126\n",
      "[MICE] Starting imputation round 87/110, elapsed time 16.304\n",
      "[MICE] Starting imputation round 88/110, elapsed time 16.460\n",
      "[MICE] Starting imputation round 89/110, elapsed time 16.641\n",
      "[MICE] Starting imputation round 90/110, elapsed time 16.798\n",
      "[MICE] Starting imputation round 91/110, elapsed time 16.973\n",
      "[MICE] Starting imputation round 92/110, elapsed time 17.260\n",
      "[MICE] Starting imputation round 93/110, elapsed time 17.418\n",
      "[MICE] Starting imputation round 94/110, elapsed time 17.645\n",
      "[MICE] Starting imputation round 95/110, elapsed time 17.857\n",
      "[MICE] Starting imputation round 96/110, elapsed time 18.047\n",
      "[MICE] Starting imputation round 97/110, elapsed time 18.230\n",
      "[MICE] Starting imputation round 98/110, elapsed time 18.392\n",
      "[MICE] Starting imputation round 99/110, elapsed time 18.573\n",
      "[MICE] Starting imputation round 100/110, elapsed time 18.735\n",
      "[MICE] Starting imputation round 101/110, elapsed time 18.916\n",
      "[MICE] Starting imputation round 102/110, elapsed time 19.088\n",
      "[MICE] Starting imputation round 103/110, elapsed time 19.274\n",
      "[MICE] Starting imputation round 104/110, elapsed time 19.434\n",
      "[MICE] Starting imputation round 105/110, elapsed time 19.630\n",
      "[MICE] Starting imputation round 106/110, elapsed time 19.844\n",
      "[MICE] Starting imputation round 107/110, elapsed time 20.025\n",
      "[MICE] Starting imputation round 108/110, elapsed time 20.208\n",
      "[MICE] Starting imputation round 109/110, elapsed time 20.369\n",
      "[MICE] Starting imputation round 110/110, elapsed time 20.554\n",
      "(1000, 172)\n",
      "[MICE] Completing matrix with shape (1000, 172)\n",
      "[MICE] Starting imputation round 1/110, elapsed time 0.004\n",
      "[MICE] Starting imputation round 2/110, elapsed time 0.202\n",
      "[MICE] Starting imputation round 3/110, elapsed time 0.429\n",
      "[MICE] Starting imputation round 4/110, elapsed time 0.620\n",
      "[MICE] Starting imputation round 5/110, elapsed time 0.824\n",
      "[MICE] Starting imputation round 6/110, elapsed time 1.024\n",
      "[MICE] Starting imputation round 7/110, elapsed time 1.249\n",
      "[MICE] Starting imputation round 8/110, elapsed time 1.450\n",
      "[MICE] Starting imputation round 9/110, elapsed time 1.660\n",
      "[MICE] Starting imputation round 10/110, elapsed time 1.848\n",
      "[MICE] Starting imputation round 11/110, elapsed time 2.053\n",
      "[MICE] Starting imputation round 12/110, elapsed time 2.242\n",
      "[MICE] Starting imputation round 13/110, elapsed time 2.465\n",
      "[MICE] Starting imputation round 14/110, elapsed time 2.655\n",
      "[MICE] Starting imputation round 15/110, elapsed time 2.861\n",
      "[MICE] Starting imputation round 16/110, elapsed time 3.051\n",
      "[MICE] Starting imputation round 17/110, elapsed time 3.259\n",
      "[MICE] Starting imputation round 18/110, elapsed time 3.449\n",
      "[MICE] Starting imputation round 19/110, elapsed time 3.655\n",
      "[MICE] Starting imputation round 20/110, elapsed time 3.845\n",
      "[MICE] Starting imputation round 21/110, elapsed time 4.051\n",
      "[MICE] Starting imputation round 22/110, elapsed time 4.241\n",
      "[MICE] Starting imputation round 23/110, elapsed time 4.471\n",
      "[MICE] Starting imputation round 24/110, elapsed time 4.662\n",
      "[MICE] Starting imputation round 25/110, elapsed time 4.867\n",
      "[MICE] Starting imputation round 26/110, elapsed time 5.058\n",
      "[MICE] Starting imputation round 27/110, elapsed time 5.263\n",
      "[MICE] Starting imputation round 28/110, elapsed time 5.453\n",
      "[MICE] Starting imputation round 29/110, elapsed time 5.662\n",
      "[MICE] Starting imputation round 30/110, elapsed time 5.851\n",
      "[MICE] Starting imputation round 31/110, elapsed time 6.059\n",
      "[MICE] Starting imputation round 32/110, elapsed time 6.250\n",
      "[MICE] Starting imputation round 33/110, elapsed time 6.483\n",
      "[MICE] Starting imputation round 34/110, elapsed time 6.679\n",
      "[MICE] Starting imputation round 35/110, elapsed time 6.887\n",
      "[MICE] Starting imputation round 36/110, elapsed time 7.076\n",
      "[MICE] Starting imputation round 37/110, elapsed time 7.283\n",
      "[MICE] Starting imputation round 38/110, elapsed time 7.470\n",
      "[MICE] Starting imputation round 39/110, elapsed time 7.678\n",
      "[MICE] Starting imputation round 40/110, elapsed time 7.868\n",
      "[MICE] Starting imputation round 41/110, elapsed time 8.076\n",
      "[MICE] Starting imputation round 42/110, elapsed time 8.267\n",
      "[MICE] Starting imputation round 43/110, elapsed time 8.496\n",
      "[MICE] Starting imputation round 44/110, elapsed time 8.685\n",
      "[MICE] Starting imputation round 45/110, elapsed time 8.891\n",
      "[MICE] Starting imputation round 46/110, elapsed time 9.081\n",
      "[MICE] Starting imputation round 47/110, elapsed time 9.289\n",
      "[MICE] Starting imputation round 48/110, elapsed time 9.488\n",
      "[MICE] Starting imputation round 49/110, elapsed time 9.706\n",
      "[MICE] Starting imputation round 50/110, elapsed time 9.897\n",
      "[MICE] Starting imputation round 51/110, elapsed time 10.104\n",
      "[MICE] Starting imputation round 52/110, elapsed time 10.294\n",
      "[MICE] Starting imputation round 53/110, elapsed time 10.532\n",
      "[MICE] Starting imputation round 54/110, elapsed time 10.721\n",
      "[MICE] Starting imputation round 55/110, elapsed time 10.928\n",
      "[MICE] Starting imputation round 56/110, elapsed time 11.116\n",
      "[MICE] Starting imputation round 57/110, elapsed time 11.322\n",
      "[MICE] Starting imputation round 58/110, elapsed time 11.513\n",
      "[MICE] Starting imputation round 59/110, elapsed time 11.720\n",
      "[MICE] Starting imputation round 60/110, elapsed time 11.910\n",
      "[MICE] Starting imputation round 61/110, elapsed time 12.117\n",
      "[MICE] Starting imputation round 62/110, elapsed time 12.306\n",
      "[MICE] Starting imputation round 63/110, elapsed time 12.540\n",
      "[MICE] Starting imputation round 64/110, elapsed time 12.727\n",
      "[MICE] Starting imputation round 65/110, elapsed time 12.933\n",
      "[MICE] Starting imputation round 66/110, elapsed time 13.121\n",
      "[MICE] Starting imputation round 67/110, elapsed time 13.326\n",
      "[MICE] Starting imputation round 68/110, elapsed time 13.516\n",
      "[MICE] Starting imputation round 69/110, elapsed time 13.721\n",
      "[MICE] Starting imputation round 70/110, elapsed time 13.925\n",
      "[MICE] Starting imputation round 71/110, elapsed time 14.132\n",
      "[MICE] Starting imputation round 72/110, elapsed time 14.321\n",
      "[MICE] Starting imputation round 73/110, elapsed time 14.558\n",
      "[MICE] Starting imputation round 74/110, elapsed time 14.749\n",
      "[MICE] Starting imputation round 75/110, elapsed time 14.956\n",
      "[MICE] Starting imputation round 76/110, elapsed time 15.145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MICE] Starting imputation round 77/110, elapsed time 15.353\n",
      "[MICE] Starting imputation round 78/110, elapsed time 15.541\n",
      "[MICE] Starting imputation round 79/110, elapsed time 15.745\n",
      "[MICE] Starting imputation round 80/110, elapsed time 15.935\n",
      "[MICE] Starting imputation round 81/110, elapsed time 16.136\n",
      "[MICE] Starting imputation round 82/110, elapsed time 16.325\n",
      "[MICE] Starting imputation round 83/110, elapsed time 16.540\n",
      "[MICE] Starting imputation round 84/110, elapsed time 16.730\n",
      "[MICE] Starting imputation round 85/110, elapsed time 16.935\n",
      "[MICE] Starting imputation round 86/110, elapsed time 17.125\n",
      "[MICE] Starting imputation round 87/110, elapsed time 17.329\n",
      "[MICE] Starting imputation round 88/110, elapsed time 17.518\n",
      "[MICE] Starting imputation round 89/110, elapsed time 17.724\n",
      "[MICE] Starting imputation round 90/110, elapsed time 17.913\n",
      "[MICE] Starting imputation round 91/110, elapsed time 18.118\n",
      "[MICE] Starting imputation round 92/110, elapsed time 18.307\n",
      "[MICE] Starting imputation round 93/110, elapsed time 18.513\n",
      "[MICE] Starting imputation round 94/110, elapsed time 18.704\n",
      "[MICE] Starting imputation round 95/110, elapsed time 18.909\n",
      "[MICE] Starting imputation round 96/110, elapsed time 19.100\n",
      "[MICE] Starting imputation round 97/110, elapsed time 19.306\n",
      "[MICE] Starting imputation round 98/110, elapsed time 19.497\n",
      "[MICE] Starting imputation round 99/110, elapsed time 19.702\n",
      "[MICE] Starting imputation round 100/110, elapsed time 19.891\n",
      "[MICE] Starting imputation round 101/110, elapsed time 20.099\n",
      "[MICE] Starting imputation round 102/110, elapsed time 20.288\n",
      "[MICE] Starting imputation round 103/110, elapsed time 20.491\n",
      "[MICE] Starting imputation round 104/110, elapsed time 20.683\n",
      "[MICE] Starting imputation round 105/110, elapsed time 20.888\n",
      "[MICE] Starting imputation round 106/110, elapsed time 21.077\n",
      "[MICE] Starting imputation round 107/110, elapsed time 21.282\n",
      "[MICE] Starting imputation round 108/110, elapsed time 21.471\n",
      "[MICE] Starting imputation round 109/110, elapsed time 21.675\n",
      "[MICE] Starting imputation round 110/110, elapsed time 21.862\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "dataframe=pd.read_csv('amputed_data_05_5.csv')\n",
    "print(dataframe.shape)\n",
    "from fancyimpute import MICE\n",
    "solver=MICE(impute_type='pmm')\n",
    "Imputed_dataframe=solver.complete(dataframe.values)\n",
    "Imputed_dataframe=pd.DataFrame(Imputed_dataframe,columns=dataframe.columns.tolist())\n",
    "Imputed_dataframe.to_csv('train_Python_05_5.csv',index=False)\n",
    "\n",
    "# 10 percent\n",
    "dataframe=pd.read_csv('amputed_data_10_5.csv')\n",
    "print(dataframe.shape)\n",
    "from fancyimpute import MICE\n",
    "solver=MICE(impute_type='pmm')\n",
    "Imputed_dataframe=solver.complete(dataframe.values)\n",
    "Imputed_dataframe=pd.DataFrame(Imputed_dataframe,columns=dataframe.columns.tolist())\n",
    "Imputed_dataframe.to_csv('train_Python_10_5.csv',index=False)\n",
    "\n",
    "#15 percent\n",
    "dataframe=pd.read_csv('amputed_data_15_5.csv')\n",
    "print(dataframe.shape)\n",
    "from fancyimpute import MICE\n",
    "solver=MICE(impute_type='pmm')\n",
    "Imputed_dataframe=solver.complete(dataframe.values)\n",
    "Imputed_dataframe=pd.DataFrame(Imputed_dataframe,columns=dataframe.columns.tolist())\n",
    "Imputed_dataframe.to_csv('train_Python_15_5.csv',index=False)\n",
    "\n",
    "#50 percent\n",
    "dataframe=pd.read_csv('amputed_data_50_5.csv')\n",
    "print(dataframe.shape)\n",
    "from fancyimpute import MICE\n",
    "solver=MICE(impute_type='pmm')\n",
    "Imputed_dataframe=solver.complete(dataframe.values)\n",
    "Imputed_dataframe=pd.DataFrame(Imputed_dataframe,columns=dataframe.columns.tolist())\n",
    "Imputed_dataframe.to_csv('train_Python_50_5.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below reads in the product of various imputes. The imputes include\n",
    "- 5 percent missing data in 5 variables. The 5 columns are the 5 most highly correlated with the target (SalePrice)\n",
    "- 10 percent in the same 5 varibles\n",
    "- 15 percent missing in the same 5 variables\n",
    "- 50 percent missing in the same 5 variables\n",
    "Because this process compares R and Python, this is a somewhat manual process. This steps include\n",
    "- We run the code below\n",
    "- we manually run the R code to create 4 (5,10,15,50) ampute files. At that times. we also create 4 files (5,10,15,50) with the imputed R values. We then run MICE in Python where the 4 amputed files are inputs. This leads to 4 output files with MICE Python imputes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to create baseline data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 172)\n",
      "1000\n",
      "449\n",
      "(449, 172)\n",
      "r2  0.8558648137139061\n"
     ]
    }
   ],
   "source": [
    "tFile=\"training_data_for_ampute.csv\"\n",
    "#tFile_test=\"\"\n",
    "## read in the training data \n",
    "t_f_subset = pd.read_csv(tFile)\n",
    "print(t_f_subset.shape)\n",
    "reg.fit(t_f_subset,b_train[split_list[0]].apply(np.log))\n",
    "#print(type(b_train[split_list[0]]))\n",
    "a_train2=df2_scaler.transform(a_train.iloc[split_list[1],:])\n",
    "a_train2=pd.DataFrame(poly.fit_transform(a_train2))\n",
    "#print(a_train.shape)\n",
    "tF=pd.DataFrame(a_train2)\n",
    "t_f_subset=get_features_filtered_on_corr(tF,tL,t_corr_thresh)\n",
    "\n",
    "tS=reg.score(t_f_subset,b_train[split_list[1]].apply(np.log))\n",
    "print(len(split_list[0]))\n",
    "print(len(split_list[1]))\n",
    "print(t_f_subset.shape)\n",
    "print('r2 ',str(tS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to predict R 5 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 172)\n",
      "(449, 30381)\n",
      "1000\n",
      "449\n",
      "(449, 172)\n",
      "r2  0.854893880620866\n"
     ]
    }
   ],
   "source": [
    "tFile=\"train_r_05_5.csv\"\n",
    "#tFile_test=\"\"\n",
    "## read in the training data \n",
    "t_f_subset = pd.read_csv(tFile)\n",
    "print(t_f_subset.shape)\n",
    "reg.fit(t_f_subset,b_train[split_list[0]].apply(np.log))\n",
    "a_train2=df2_scaler.transform(a_train.iloc[split_list[1],:])\n",
    "a_train2=pd.DataFrame(poly.fit_transform(a_train2))\n",
    "#print(a_train.shape)\n",
    "tF=pd.DataFrame(a_train2)\n",
    "print(tF.shape)\n",
    "t_f_subset=get_features_filtered_on_corr(tF,tL,t_corr_thresh)\n",
    "\n",
    "tS=reg.score(t_f_subset,b_train[split_list[1]].apply(np.log))\n",
    "print(len(split_list[0]))\n",
    "print(len(split_list[1]))\n",
    "print(t_f_subset.shape)\n",
    "print('r2 ',str(tS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to predict R 10 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "449\n",
      "(449, 172)\n",
      "r2  0.8587977616010477\n"
     ]
    }
   ],
   "source": [
    "tFile=\"train_r_10_5.csv\"\n",
    "#tFile_test=\"\"\n",
    "## read in the training data \n",
    "t_f_subset = pd.read_csv(tFile)\n",
    "reg.fit(t_f_subset,b_train[split_list[0]].apply(np.log))\n",
    "a_train2=df2_scaler.transform(a_train.iloc[split_list[1],:])\n",
    "a_train2=pd.DataFrame(poly.fit_transform(a_train2))\n",
    "#print(a_train.shape)\n",
    "tF=pd.DataFrame(a_train2)\n",
    "t_f_subset=get_features_filtered_on_corr(tF,tL,t_corr_thresh)\n",
    "\n",
    "tS=reg.score(t_f_subset,b_train[split_list[1]].apply(np.log))\n",
    "print(len(split_list[0]))\n",
    "print(len(split_list[1]))\n",
    "print(t_f_subset.shape)\n",
    "print('r2 ',str(tS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to predict R 15 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "449\n",
      "(449, 172)\n",
      "r2  0.8579570838639352\n"
     ]
    }
   ],
   "source": [
    "tFile=\"train_r_15_5.csv\"\n",
    "#tFile_test=\"\"\n",
    "## read in the training data \n",
    "t_f_subset = pd.read_csv(tFile)\n",
    "reg.fit(t_f_subset,b_train[split_list[0]].apply(np.log))\n",
    "a_train2=df2_scaler.transform(a_train.iloc[split_list[1],:])\n",
    "a_train2=pd.DataFrame(poly.fit_transform(a_train2))\n",
    "#print(a_train.shape)\n",
    "tF=pd.DataFrame(a_train2)\n",
    "t_f_subset=get_features_filtered_on_corr(tF,tL,t_corr_thresh)\n",
    "\n",
    "tS=reg.score(t_f_subset,b_train[split_list[1]].apply(np.log))\n",
    "print(len(split_list[0]))\n",
    "print(len(split_list[1]))\n",
    "print(t_f_subset.shape)\n",
    "print('r2 ',str(tS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to predict R 50 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "449\n",
      "(449, 172)\n",
      "r2  0.8620359357467404\n"
     ]
    }
   ],
   "source": [
    "tFile=\"train_r_50_5.csv\"\n",
    "#tFile_test=\"\"\n",
    "## read in the training data \n",
    "t_f_subset = pd.read_csv(tFile)\n",
    "reg.fit(t_f_subset,b_train[split_list[0]].apply(np.log))\n",
    "a_train2=df2_scaler.transform(a_train.iloc[split_list[1],:])\n",
    "a_train2=pd.DataFrame(poly.fit_transform(a_train2))\n",
    "#print(a_train.shape)\n",
    "tF=pd.DataFrame(a_train2)\n",
    "t_f_subset=get_features_filtered_on_corr(tF,tL,t_corr_thresh)\n",
    "\n",
    "tS=reg.score(t_f_subset,b_train[split_list[1]].apply(np.log))\n",
    "print(len(split_list[0]))\n",
    "print(len(split_list[1]))\n",
    "print(t_f_subset.shape)\n",
    "print('r2 ',str(tS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Code to predict Python 5 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "449\n",
      "(449, 172)\n",
      "r2  0.8554178202493401\n"
     ]
    }
   ],
   "source": [
    "tFile=\"train_Python_05_5.csv\"\n",
    "#tFile_test=\"\"\n",
    "## read in the training data \n",
    "t_f_subset = pd.read_csv(tFile)\n",
    "reg.fit(t_f_subset,b_train[split_list[0]].apply(np.log))\n",
    "a_train2=df2_scaler.transform(a_train.iloc[split_list[1],:])\n",
    "a_train2=pd.DataFrame(poly.fit_transform(a_train2))\n",
    "#print(a_train.shape)\n",
    "tF=pd.DataFrame(a_train2)\n",
    "t_f_subset=get_features_filtered_on_corr(tF,tL,t_corr_thresh)\n",
    "\n",
    "tS=reg.score(t_f_subset,b_train[split_list[1]].apply(np.log))\n",
    "print(len(split_list[0]))\n",
    "print(len(split_list[1]))\n",
    "print(t_f_subset.shape)\n",
    "print('r2 ',str(tS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Code to predict Python 10 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "449\n",
      "(449, 172)\n",
      "r2  0.8586030951535819\n"
     ]
    }
   ],
   "source": [
    "tFile=\"train_Python_10_5.csv\"\n",
    "#tFile_test=\"\"\n",
    "## read in the training data \n",
    "t_f_subset = pd.read_csv(tFile)\n",
    "reg.fit(t_f_subset,b_train[split_list[0]].apply(np.log))\n",
    "a_train2=df2_scaler.transform(a_train.iloc[split_list[1],:])\n",
    "a_train2=pd.DataFrame(poly.fit_transform(a_train2))\n",
    "#print(a_train.shape)\n",
    "tF=pd.DataFrame(a_train2)\n",
    "t_f_subset=get_features_filtered_on_corr(tF,tL,t_corr_thresh)\n",
    "\n",
    "tS=reg.score(t_f_subset,b_train[split_list[1]].apply(np.log))\n",
    "print(len(split_list[0]))\n",
    "print(len(split_list[1]))\n",
    "print(t_f_subset.shape)\n",
    "print('r2 ',str(tS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Code to predict Python 15 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "449\n",
      "(449, 172)\n",
      "r2  0.852915146879226\n"
     ]
    }
   ],
   "source": [
    "tFile=\"train_Python_15_5.csv\"\n",
    "#tFile_test=\"\"\n",
    "## read in the training data \n",
    "t_f_subset = pd.read_csv(tFile)\n",
    "reg.fit(t_f_subset,b_train[split_list[0]].apply(np.log))\n",
    "a_train2=df2_scaler.transform(a_train.iloc[split_list[1],:])\n",
    "a_train2=pd.DataFrame(poly.fit_transform(a_train2))\n",
    "#print(a_train.shape)\n",
    "tF=pd.DataFrame(a_train2)\n",
    "t_f_subset=get_features_filtered_on_corr(tF,tL,t_corr_thresh)\n",
    "\n",
    "tS=reg.score(t_f_subset,b_train[split_list[1]].apply(np.log))\n",
    "print(len(split_list[0]))\n",
    "print(len(split_list[1]))\n",
    "print(t_f_subset.shape)\n",
    "print('r2 ',str(tS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Code to predict Python 50 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t_f_subset = pd.read_csv(tFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "449\n",
      "(449, 172)\n",
      "r2  0.8504106885207705\n"
     ]
    }
   ],
   "source": [
    "tFile=\"train_Python_50_5.csv\"\n",
    "#tFile_test=\"\"\n",
    "## read in the training data \n",
    "t_f_subset = pd.read_csv(tFile)\n",
    "reg.fit(t_f_subset,b_train[split_list[0]].apply(np.log))\n",
    "a_train2=df2_scaler.transform(a_train.iloc[split_list[1],:])\n",
    "a_train2=pd.DataFrame(poly.fit_transform(a_train2))\n",
    "#print(a_train.shape)\n",
    "tF=pd.DataFrame(a_train2)\n",
    "t_f_subset=get_features_filtered_on_corr(tF,tL,t_corr_thresh)\n",
    "\n",
    "tS=reg.score(t_f_subset,b_train[split_list[1]].apply(np.log))\n",
    "print(len(split_list[0]))\n",
    "print(len(split_list[1]))\n",
    "print(t_f_subset.shape)\n",
    "print('r2 ',str(tS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "449\n",
      "(449, 172)\n",
      "r2  0.8521207289054102\n"
     ]
    }
   ],
   "source": [
    "tFile=\"train_impute_mean_50.csv\"\n",
    "#tFile_test=\"\"\n",
    "## read in the training data \n",
    "t_f_subset = pd.read_csv(tFile)\n",
    "reg.fit(t_f_subset,b_train[split_list[0]].apply(np.log))\n",
    "a_train2=df2_scaler.transform(a_train.iloc[split_list[1],:])\n",
    "a_train2=pd.DataFrame(poly.fit_transform(a_train2))\n",
    "#print(a_train.shape)\n",
    "tF=pd.DataFrame(a_train2)\n",
    "t_f_subset=get_features_filtered_on_corr(tF,tL,t_corr_thresh)\n",
    "\n",
    "tS=reg.score(t_f_subset,b_train[split_list[1]].apply(np.log))\n",
    "print(len(split_list[0]))\n",
    "print(len(split_list[1]))\n",
    "print(t_f_subset.shape)\n",
    "print('r2 ',str(tS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
