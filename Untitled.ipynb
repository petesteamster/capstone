{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem of Missing Data \n",
    "Below is a partial data set used to predict sales at a hardware store for a month. The features are ***Rain (1=yes,0=no),Cloudy (1=yes,0=no),Temperature,Customers in a 24 hour period***. The features are used in an attempt to see how weather affects sales. Note that some of the data is flagged as missing. A predictive model cannot be built if we do not fill in the missing data with values. Also, because collecting data takes time and money, we dont want to toss the data out. What values should we use to fill in the missing data?\n",
    "\n",
    "<p align=\"left\">\n",
    "  <img src=\"missing_data01.png\" height=\"300\" width=\"360\">\n",
    "</p>\n",
    "\n",
    "## Impute Missing Data\n",
    "The term for guessing/replacing a value for the missing data is **imputation**. A simple imputation method is to calculate the mean or the median of a column that has missing data and then set all the missing data in that column to the value of the calcuation. Of course, the calculation is only performed on the values in the column that are *not missing*. Although this method is used all the time, it has some flaws. A major flaw is it artificailly reduces the variance. This can decrease the size of confidence intervals which can lead to making poor inferences. \n",
    "## MICE\n",
    "MICE is an acronym for *Multivariate Imputation by Chained Equations*. It is an algorithm that relies on regression methods to perform imputations. Below we provide a high level/stepwise description of how MICE works.  \n",
    "   ### *Step 1*\n",
    "   Below is the partial dataset with only features. We use the *mean impute* method on all the missing values except 1. This is illustrated in step 2.  \n",
    "<p align=\"left\">\n",
    "  <img src=\"missing_data01_B.png\" height=\"300\" width=\"300\">\n",
    "</p>\n",
    "  \n",
    "  ### *Step 2*\n",
    "  In the dateset below, the missing values in *Customer* and *Temperature* have been replaced with the column mean. The missing value in \n",
    "  *Rain* was left *missing*. Now *Rain* can be imputed using a regression method. This is illustrated in step 3. \n",
    "<p align=\"left\">\n",
    "  <img src=\"missing_data02_B.png\" height=\"300\" width=\"300\">\n",
    "</p>\n",
    "\n",
    "### *Step 3*\n",
    "  In this step, the regression prediction replaced the missing value in *Rain*. Also, the mean impute in *Customers* has been set back to *Missing*. Note that *Temperature* was left alone. A regresson is run the imputes a new value for the missing value in *Customers*. This is shown in step 4. \n",
    "<p align=\"left\">\n",
    "  <img src=\"missing_data03_B.png\" height=\"300\" width=\"300\">\n",
    "</p>\n",
    "\n",
    "### *Step 4*\n",
    "  In this step, the regression prediction replaced the missing value in *Customers*. Also, the mean impute in *Temperature* has been set back to *Missing*. Note that *Rain* was left alone. A regresson is run the imputes a new value for the missing value in *Temperature*. This is shown in step 5. \n",
    "<p align=\"left\">\n",
    "  <img src=\"missing_data04_B.png\" height=\"300\" width=\"300\">\n",
    "</p>\n",
    " \n",
    " ### *Step 5*\n",
    "  In this step, the regression prediction replaced the missing value in *Temperature*. All the missing values have been replaced by a regression impute. This is run several times. For example, *Customers* might be set to *Missing* again and the regression rerun. Then *Rain* is reset. Etc. The process can go on for several iterations. This leads to imputed values that do not cause problems with variance reduction and so on. \n",
    "<p align=\"left\">\n",
    "  <img src=\"missing_data05_B.png\" height=\"300\" width=\"300\">\n",
    "</p>\n",
    "\n",
    "  ### Repeat the Steps from the top. \n",
    "  When the steps are repeated, a new dataset is created with a new set of imputed values. The repeat process is done several time. The default for R is 5. A model is built on each of these data sets and the results are combined. The rules for combining to models are decribed by a paper from 1987 by Rubin. A copy of the paper can be found in the repo. The title is *Rubin's Rules*. Luckily, R and Python have functions that do all the calculations described in *Rubin's Rules*.     \n",
    "  \n",
    "  # Goal: Compare the R and the Python implemtation of MICE\n",
    "  \n",
    "  - Now that we have described the MICE algorithm, we can get to the goal of the project. We want to evaluate two implemenatations of  MICE. One implemenatation is written for R. It has been around since 2000. The other implementation is written for Python. It is much more recent than the R version.\n",
    "\n",
    "\n",
    "# Implementation and Analysis\n",
    "\n",
    "- To test, we simulated data with binary, poisson, ordinal and normal data. We wanted to see how well the imputed data matched the 'actual' i.e. simulated data. We decided to process all normal data such that it skewed to the right or left. We wanted continuous data with some variety in shape. \n",
    "Each set of simulated data had 10 columns. We did some calculations and found that there are 84 different ways to mix the four datatypes. For example, one combination might be 2 poisson (2p), 3 binary (3b), 2 ordinal (2o) and 3 normal (3n). Another might be 4p, 4b, 1b and 1n. Etc. This leads to 84 different combinations. Also, the probabilties for the binary and ordinals can vary. The mean for the poisson can vary as can the mean and the standard deviation for the normal. Another variable is the correlation matrix used for each simulation. We decided to simulate 168 sets of data - 2 for each of the 84 combinations.  \n",
    "- We found that Python Mice does not handle discrete data well. It imputes fractional values for binary, ordinal and poisson data. On the other hand, R Mice imputes data with the correct data type. Since Python essentially fails when imputing descrete data, we decided there was no need to go further. That is, we did not perform a statistical analysis comparing R and Python for descrete data.\n",
    "- We performed extensive statistical analysis in our comparison of how R and Python MICE handle consinuous data. We compared density curves visually and numerically. To do the numeric comparisions, we used techniques from functional data analysis (we used the R package fdasrvf) to calculate a metric used to measure how simular two density curves (i.e. functions) are to each other. We compared the density curves of the Python and the R imputations to the actual values they replaced. We had enough data to perform a t-test. The results are shown in the table below.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
